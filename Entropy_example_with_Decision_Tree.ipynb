{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrhMeXX4ZvvQA/8Z/C5m2S"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRgW69QSKwYL",
        "outputId": "ee5a5994-29cc-4e74-96bf-84c092069e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 1 0 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Calculate entropy\n",
        "def entropy(y):\n",
        "    class_counts = np.bincount(y)\n",
        "    probabilities = class_counts / len(y)\n",
        "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
        "\n",
        "# Calculate information gain\n",
        "def information_gain(X, y, feature_index, threshold):\n",
        "    parent_entropy = entropy(y)\n",
        "\n",
        "    # Split dataset\n",
        "    left_indices = X[:, feature_index] <= threshold\n",
        "    right_indices = X[:, feature_index] > threshold\n",
        "\n",
        "    if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
        "        return 0\n",
        "\n",
        "    # Weighted entropy of child nodes\n",
        "    n = len(y)\n",
        "    n_left = sum(left_indices)\n",
        "    n_right = sum(right_indices)\n",
        "\n",
        "    left_entropy = entropy(y[left_indices])\n",
        "    right_entropy = entropy(y[right_indices])\n",
        "\n",
        "    child_entropy = (n_left / n) * left_entropy + (n_right / n) * right_entropy\n",
        "\n",
        "    # Information gain\n",
        "    return parent_entropy - child_entropy\n",
        "\n",
        "# Decision Tree Node\n",
        "class DecisionNode:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value\n",
        "\n",
        "# Decision Tree Classifier\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=5, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        n_samples, n_features = X.shape\n",
        "        if n_samples < self.min_samples_split or depth >= self.max_depth or len(set(y)) == 1:\n",
        "            leaf_value = np.bincount(y).argmax()\n",
        "            return DecisionNode(value=leaf_value)\n",
        "\n",
        "        # Find best split\n",
        "        best_gain = -1\n",
        "        best_feature_index, best_threshold = None, None\n",
        "        for feature_index in range(n_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                gain = information_gain(X, y, feature_index, threshold)\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        if best_gain == -1:\n",
        "            leaf_value = np.bincount(y).argmax()\n",
        "            return DecisionNode(value=leaf_value)\n",
        "\n",
        "        # Recursively build left and right subtrees\n",
        "        left_indices = X[:, best_feature_index] <= best_threshold\n",
        "        right_indices = X[:, best_feature_index] > best_threshold\n",
        "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
        "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
        "        return DecisionNode(best_feature_index, best_threshold, left_subtree, right_subtree)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y, 0)\n",
        "\n",
        "    def _predict(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature_index] <= node.threshold:\n",
        "            return self._predict(x, node.left)\n",
        "        else:\n",
        "            return self._predict(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x, self.root) for x in X])\n",
        "\n",
        "# Sample Data\n",
        "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0], [1, 1], [0, 0]])\n",
        "y = np.array([1, 1, 0, 0, 1, 0])\n",
        "\n",
        "# Train the decision tree\n",
        "tree = DecisionTree(max_depth=2)\n",
        "tree.fit(X, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = tree.predict(X)\n",
        "print(\"Predictions:\", predictions)\n"
      ]
    }
  ]
}